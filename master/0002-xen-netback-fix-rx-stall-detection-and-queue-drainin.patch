From 4d23e36de74e41386d9dbe841de8011e4350e3f0 Mon Sep 17 00:00:00 2001
From: David Vrabel <david.vrabel@citrix.com>
Date: Fri, 29 Aug 2014 19:08:05 +0100
Subject: [PATCH 2/2] xen-netback: fix rx stall detection and queue draining

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 2bb2dd1..13670c8 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -100,13 +100,14 @@ struct xenvif_rx_meta {
  */
 #define XEN_NETBK_LEGACY_SLOTS_MAX XEN_NETIF_NR_SLOTS_MIN
 
-enum state_bit_shift {
+/* Maximum number of Rx slots a to-guest packet may use, including the
+ * slot needed for GSO meta-data.
+ */
+#define XEN_NETBK_RX_SLOTS_MAX (MAX_SKB_FRAGS + 1)
+
+enum xenvif_state_bit_shift {
 	/* This bit marks that the vif is connected */
 	VIF_STATUS_CONNECTED,
-	/* This bit signals the RX thread that queuing was stopped (in
-	 * start_xmit), and either the timer fired or an RX interrupt came
-	 */
-	VIF_STATUS_RX_PURGE_EVENT
 };
 
 struct xenvif {
@@ -164,9 +165,9 @@ struct xenvif {
 	char rx_irq_name[IFNAMSIZ+4]; /* DEVNAME-rx */
 	struct xen_netif_rx_back_ring rx;
 	struct sk_buff_head rx_queue;
-	RING_IDX rx_last_skb_slots;
-
-	struct timer_list rx_stalled;
+	unsigned int rx_queue_max;
+	unsigned int rx_queue_len;
+	unsigned long last_rx_time;
 
 	/* This array is allocated seperately as it is large */
 	struct gnttab_copy *grant_copy_op;
@@ -213,6 +214,14 @@ static inline struct xenbus_device *xenvif_to_xenbus_device(struct xenvif *vif)
 	return to_xenbus_device(vif->dev->dev.parent);
 }
 
+struct xenvif_rx_cb {
+	unsigned long expires;
+	int meta_slots_used;
+	bool full_coalesce;
+};
+
+#define XENVIF_RX_CB(skb) ((struct xenvif_rx_cb *)(skb)->cb)
+
 struct xenvif *xenvif_alloc(struct device *parent,
 			    domid_t domid,
 			    unsigned int handle);
@@ -252,6 +261,8 @@ int xenvif_dealloc_kthread(void *data);
  */
 bool xenvif_rx_ring_slots_available(struct xenvif *vif, int needed);
 
+void xenvif_rx_queue_tail(struct xenvif *vif, struct sk_buff *skb);
+
 /* Callback from stack when TX packet can be released */
 void xenvif_zerocopy_callback(struct ubuf_info *ubuf, bool zerocopy_success);
 
diff --git a/drivers/net/xen-netback/interface.c b/drivers/net/xen-netback/interface.c
index 0c73a8b..bdef8de 100644
--- a/drivers/net/xen-netback/interface.c
+++ b/drivers/net/xen-netback/interface.c
@@ -43,10 +43,14 @@
 #define XENVIF_QUEUE_LENGTH 32
 #define XENVIF_NAPI_WEIGHT  64
 
+/* Number of bytes allowed on the internal guest Rx queue. */
+#define XENVIF_RX_QUEUE_BYTES (XEN_NETIF_RX_RING_SIZE/2 * PAGE_SIZE)
+
 int xenvif_schedulable(struct xenvif *vif)
 {
 	return netif_running(vif->dev) &&
-		test_bit(VIF_STATUS_CONNECTED, &vif->status);
+		test_bit(VIF_STATUS_CONNECTED, &vif->status) &&
+		!vif->disabled;
 }
 
 static irqreturn_t xenvif_tx_interrupt(int irq, void *dev_id)
@@ -87,8 +91,6 @@ static irqreturn_t xenvif_rx_interrupt(int irq, void *dev_id)
 {
 	struct xenvif *vif = dev_id;
 
-	if (unlikely(netif_queue_stopped(vif->dev) || !netif_carrier_ok(vif->dev)))
-		set_bit(VIF_STATUS_RX_PURGE_EVENT, &vif->status);
 	xenvif_kick_thread(vif);
 
 	return IRQ_HANDLED;
@@ -102,20 +104,10 @@ irqreturn_t xenvif_interrupt(int irq, void *dev_id)
 	return IRQ_HANDLED;
 }
 
-static void xenvif_rx_stalled(unsigned long data)
-{
-	struct xenvif *vif = (struct xenvif *)data;
-
-	if (netif_queue_stopped(vif->dev)) {
-		set_bit(VIF_STATUS_RX_PURGE_EVENT, &vif->status);
-		xenvif_kick_thread(vif);
-	}
-}
-
 static int xenvif_start_xmit(struct sk_buff *skb, struct net_device *dev)
 {
 	struct xenvif *vif = netdev_priv(dev);
-	int min_slots_needed;
+	struct xenvif_rx_cb *cb;
 
 	BUG_ON(skb->dev != dev);
 
@@ -125,30 +117,10 @@ static int xenvif_start_xmit(struct sk_buff *skb, struct net_device *dev)
 	    !xenvif_schedulable(vif))
 		goto drop;
 
-	/* At best we'll need one slot for the header and one for each
-	 * frag.
-	 */
-	min_slots_needed = 1 + skb_shinfo(skb)->nr_frags;
+	cb = XENVIF_RX_CB(skb);
+	cb->expires = jiffies + rx_drain_timeout_jiffies;
 
-	/* If the skb is GSO then we'll also need an extra slot for the
-	 * metadata.
-	 */
-	if (skb_is_gso(skb))
-		min_slots_needed++;
-
-	/* If the skb can't possibly fit in the remaining slots
-	 * then turn off the queue to give the ring a chance to
-	 * drain.
-	 */
-	if (!xenvif_rx_ring_slots_available(vif, min_slots_needed)) {
-		vif->rx_stalled.function = xenvif_rx_stalled;
-		vif->rx_stalled.data = (unsigned long)vif;
-		xenvif_stop_queue(vif);
-		mod_timer(&vif->rx_stalled,
-			jiffies + rx_drain_timeout_jiffies);
-	}
-
-	skb_queue_tail(&vif->rx_queue, skb);
+	xenvif_rx_queue_tail(vif, skb);
 	xenvif_kick_thread(vif);
 
 	return NETDEV_TX_OK;
@@ -356,8 +328,6 @@ struct xenvif *xenvif_alloc(struct device *parent, domid_t domid,
 	init_timer(&vif->credit_timeout);
 	vif->credit_window_start = get_jiffies_64();
 
-	init_timer(&vif->rx_stalled);
-
 	dev->netdev_ops	= &xenvif_netdev_ops;
 	dev->hw_features = NETIF_F_SG |
 		NETIF_F_IP_CSUM | NETIF_F_IPV6_CSUM |
@@ -367,6 +337,8 @@ struct xenvif *xenvif_alloc(struct device *parent, domid_t domid,
 
 	dev->tx_queue_len = XENVIF_QUEUE_LENGTH;
 
+	vif->rx_queue_max = XENVIF_RX_QUEUE_BYTES;
+
 	skb_queue_head_init(&vif->rx_queue);
 	skb_queue_head_init(&vif->tx_queue);
 
@@ -472,6 +444,8 @@ int xenvif_connect(struct xenvif *vif, unsigned long tx_ring_ref,
 		disable_irq(vif->rx_irq);
 	}
 
+	vif->last_rx_time = jiffies; /* Reset Rx stall detection. */
+
 	task = kthread_create(xenvif_kthread_guest_rx,
 			      (void *)vif, "%s-guest-rx", vif->dev->name);
 	if (IS_ERR(task)) {
@@ -538,7 +512,6 @@ void xenvif_disconnect(struct xenvif *vif)
 	xenvif_carrier_off(vif);
 
 	if (vif->task) {
-		del_timer_sync(&vif->rx_stalled);
 		kthread_stop(vif->task);
 		vif->task = NULL;
 	}
diff --git a/drivers/net/xen-netback/netback.c b/drivers/net/xen-netback/netback.c
index d0f58be..76b02e4 100644
--- a/drivers/net/xen-netback/netback.c
+++ b/drivers/net/xen-netback/netback.c
@@ -55,13 +55,20 @@
 bool separate_tx_rx_irq = 1;
 module_param(separate_tx_rx_irq, bool, 0644);
 
-/* When guest ring is filled up, qdisc queues the packets for us, but we have
- * to timeout them, otherwise other guests' packets can get stuck there
+/* The time that packets can stay on the guest Rx internal queue
+ * before they are dropped.
  */
 unsigned int rx_drain_timeout_msecs = 10000;
 module_param(rx_drain_timeout_msecs, uint, 0444);
 unsigned int rx_drain_timeout_jiffies;
 
+/* The length of time before the frontend is considered unresponsive
+ * because it isn't providing Rx slots.
+ */
+static unsigned int rx_stall_timeout_msecs = 60000;
+module_param(rx_stall_timeout_msecs, uint, 0444);
+static unsigned int rx_stall_timeout_jiffies;
+
 /*
  * This is the maximum slots a skb can have. If a guest sends a skb
  * which exceeds this limit it is considered malicious.
@@ -78,7 +85,6 @@ static void make_tx_response(struct xenvif *vif,
 			     s8       st);
 
 static inline int tx_work_todo(struct xenvif *vif);
-static inline int rx_work_todo(struct xenvif *vif);
 
 static struct xen_netif_rx_response *make_rx_response(struct xenvif *vif,
 					     u16      id,
@@ -158,6 +164,69 @@ bool xenvif_rx_ring_slots_available(struct xenvif *vif, int needed)
 	return false;
 }
 
+void xenvif_rx_queue_tail(struct xenvif *vif, struct sk_buff *skb)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&vif->rx_queue.lock, flags);
+
+	__skb_queue_tail(&vif->rx_queue, skb);
+
+	vif->rx_queue_len += skb->len;
+	if (vif->rx_queue_len > vif->rx_queue_max)
+		netif_stop_queue(vif->dev);
+
+	spin_unlock_irqrestore(&vif->rx_queue.lock, flags);
+}
+
+static struct sk_buff *xenvif_rx_dequeue(struct xenvif *vif)
+{
+	struct sk_buff *skb;
+
+	spin_lock_irq(&vif->rx_queue.lock);
+
+	skb = __skb_dequeue(&vif->rx_queue);
+	if (skb)
+		vif->rx_queue_len -= skb->len;
+
+	spin_unlock_irq(&vif->rx_queue.lock);
+
+	return skb;
+}
+
+static void xenvif_rx_queue_maybe_wake(struct xenvif *vif)
+{
+	spin_lock_irq(&vif->rx_queue.lock);
+
+	if (vif->rx_queue_len < vif->rx_queue_max)
+		netif_wake_queue(vif->dev);
+
+	spin_unlock_irq(&vif->rx_queue.lock);
+}
+
+
+static void xenvif_rx_queue_purge(struct xenvif *vif)
+{
+	struct sk_buff *skb;
+	while ((skb = xenvif_rx_dequeue(vif)) != NULL)
+		kfree_skb(skb);
+}
+
+static void xenvif_rx_queue_drop_expired(struct xenvif *vif)
+{
+	struct sk_buff *skb;
+
+	for(;;) {
+		skb = skb_peek(&vif->rx_queue);
+		if (!skb)
+			break;
+		if (time_before(jiffies, XENVIF_RX_CB(skb)->expires))
+			break;
+		xenvif_rx_dequeue(vif);
+		kfree_skb(skb);
+	}
+}
+
 /*
  * Returns true if we should start a new receive buffer instead of
  * adding 'size' bytes to a buffer which currently contains 'offset'
@@ -232,13 +301,6 @@ static struct xenvif_rx_meta *get_next_rx_buffer(struct xenvif *vif,
 	return meta;
 }
 
-struct xenvif_rx_cb {
-	int meta_slots_used;
-	bool full_coalesce;
-};
-
-#define XENVIF_RX_CB(skb) ((struct xenvif_rx_cb *)(skb)->cb)
-
 /*
  * Set up the grant operations for this fragment. If it's a flipping
  * interface, we also set up the unmap request from here.
@@ -580,12 +642,15 @@ static void xenvif_rx_action(struct xenvif *vif)
 
 	skb_queue_head_init(&rxq);
 
-	while ((skb = skb_dequeue(&vif->rx_queue)) != NULL) {
+	while (xenvif_rx_ring_slots_available(vif, XEN_NETBK_RX_SLOTS_MAX)
+	       && (skb = xenvif_rx_dequeue(vif)) != NULL) {
 		RING_IDX max_slots_needed;
 		RING_IDX old_req_cons;
 		RING_IDX ring_slots_used;
 		int i;
 
+		vif->last_rx_time = jiffies;
+
 		/* We need a cheap worse case estimate for the number of
 		 * slots we'll use.
 		 */
@@ -627,15 +692,6 @@ static void xenvif_rx_action(struct xenvif *vif)
 		    skb_shinfo(skb)->gso_type & SKB_GSO_TCPV6))
 			max_slots_needed++;
 
-		/* If the skb may not fit then bail out now */
-		if (!xenvif_rx_ring_slots_available(vif, max_slots_needed)) {
-			skb_queue_head(&vif->rx_queue, skb);
-			need_to_notify = true;
-			vif->rx_last_skb_slots = max_slots_needed;
-			break;
-		} else
-			vif->rx_last_skb_slots = 0;
-
 		old_req_cons = vif->rx.req_cons;
 		XENVIF_RX_CB(skb)->meta_slots_used = xenvif_gop_skb(skb, &npo);
 		BUG_ON(XENVIF_RX_CB(skb)->meta_slots_used > max_slots_needed);
@@ -1856,12 +1912,6 @@ void xenvif_idx_unmap(struct xenvif *vif, u16 pending_idx)
 	}
 }
 
-static inline int rx_work_todo(struct xenvif *vif)
-{
-	return !skb_queue_empty(&vif->rx_queue) &&
-		xenvif_rx_ring_slots_available(vif, vif->rx_last_skb_slots);
-}
-
 static inline int tx_work_todo(struct xenvif *vif)
 {
 
@@ -1919,69 +1969,93 @@ err:
 	return err;
 }
 
-static void xenvif_start_queue(struct xenvif *vif)
+static bool xenvif_rx_queue_stalled(struct xenvif *vif)
 {
-	if (xenvif_schedulable(vif))
-		netif_wake_queue(vif->dev);
+	RING_IDX prod, cons;
+
+	prod = vif->rx.sring->req_prod;
+	cons = vif->rx.req_cons;
+
+	return netif_carrier_ok(vif->dev)
+		&& prod - cons < XEN_NETBK_RX_SLOTS_MAX
+		&& time_after(jiffies,
+			      vif->last_rx_time + rx_stall_timeout_jiffies);
 }
 
-/* Only called from the queue's thread, it handles the situation when the guest
- * doesn't post enough requests on the receiving ring.
- * First xenvif_start_xmit disables QDisc and start a timer, and then either the
- * timer fires, or the guest send an interrupt after posting new request. If it
- * is the timer, the carrier is turned off here.
- * */
-static void xenvif_rx_purge_event(struct xenvif *vif)
+static bool xenvif_rx_queue_ready(struct xenvif *vif)
 {
-	/* Either the last unsuccesful skb or at least 1 slot should fit */
-	int needed = vif->rx_last_skb_slots ?
-		     vif->rx_last_skb_slots : 1;
-
-	if (!xenvif_rx_ring_slots_available(vif, needed)) {
-		/* It is assumed that if the guest post new slots after this,
-		 * the RX interrupt will set the VIF_STATUS_RX_PURGE_EVENT bit
-		 * and wake up the thread again
-		 */
-		rtnl_lock();
-		if (netif_carrier_ok(vif->dev)) {
-			/* Timer fired and there are still no slots. Turn off
-			 * everything except the interrupts
-			 */
-			netif_carrier_off(vif->dev);
-			skb_queue_purge(&vif->rx_queue);
-			vif->rx_last_skb_slots = 0;
-			if (net_ratelimit())
-				netdev_err(vif->dev, "Carrier off due to lack of guest response\n");
-		}
-		rtnl_unlock();
-	} else if (!netif_carrier_ok(vif->dev)) {
-		/* The carrier was down, but an interrupt kicked the thread
-		 * again after new requests were posted
-		 */
-		rtnl_lock();
-		netif_carrier_on(vif->dev);
-		xenvif_start_queue(vif);
-		rtnl_unlock();
-		napi_schedule(&vif->napi);
-		if (net_ratelimit())
-			netdev_err(vif->dev, "Carrier on again\n");
-	} else {
-		/* Queuing were stopped, but the guest posted new requests */
-		del_timer_sync(&vif->rx_stalled);
-		xenvif_start_queue(vif);
+	RING_IDX prod, cons;
+
+	prod = vif->rx.sring->req_prod;
+	cons = vif->rx.req_cons;
+
+	return xenvif_schedulable(vif)
+		&& !netif_carrier_ok(vif->dev)
+		&& prod - cons >= XEN_NETBK_RX_SLOTS_MAX;
+}
+
+static bool xenvif_have_rx_work(struct xenvif *vif)
+{
+	return (!skb_queue_empty(&vif->rx_queue)
+		&& xenvif_rx_ring_slots_available(vif, XEN_NETBK_RX_SLOTS_MAX))
+		|| xenvif_rx_queue_stalled(vif)
+		|| xenvif_rx_queue_ready(vif)
+		|| kthread_should_stop()
+		|| vif->disabled;
+}
+
+static long xenvif_rx_queue_timeout(struct xenvif *vif)
+{
+	struct sk_buff *skb;
+	long timeout;
+
+	skb = skb_peek(&vif->rx_queue);
+	if (!skb)
+		return MAX_SCHEDULE_TIMEOUT;
+
+	timeout = XENVIF_RX_CB(skb)->expires - jiffies;
+	return timeout < 0 ? 0 : timeout;
+}
+
+/* Wait until the guest Rx thread has work.
+ *
+ * The timeout needs to be adjusted based on the current head of the
+ * queue (and not just the head at the beginning).  In particular, if
+ * the queue is initially empty an infinite timeout is used and this
+ * needs to be reduced when a skb is queued.
+ * 
+ * This cannot be done with wait_event_timeout() because it only
+ * calculates the timeout once.
+ */
+static void xenvif_wait_for_rx_work(struct xenvif *vif)
+{
+	DEFINE_WAIT(wait);
+
+	if (xenvif_have_rx_work(vif))
+		return;
+
+	for (;;) {
+		long ret;
+
+		prepare_to_wait(&vif->wq, &wait, TASK_INTERRUPTIBLE);
+		if (xenvif_have_rx_work(vif))
+			break;
+		ret = schedule_timeout(xenvif_rx_queue_timeout(vif));
+		if (!ret)
+			break;
 	}
+	finish_wait(&vif->wq, &wait); 
 }
+
 int xenvif_kthread_guest_rx(void *data)
 {
 	struct xenvif *vif = data;
-	struct sk_buff *skb;
 
-	while (!kthread_should_stop()) {
-		wait_event_interruptible(vif->wq,
-					 rx_work_todo(vif) ||
-					 vif->disabled ||
-					 test_bit(VIF_STATUS_RX_PURGE_EVENT, &vif->status) ||
-					 kthread_should_stop());
+	for (;;) {
+		xenvif_wait_for_rx_work(vif);
+
+		if (kthread_should_stop())
+			break;
 
 		/* This frontend is found to be rogue, disable it in
 		 * kthread context. Currently this is only set when
@@ -1989,24 +2063,43 @@ int xenvif_kthread_guest_rx(void *data)
 		 * but we cannot disable the interface in softirq
 		 * context so we defer it here.
 		 */
-		if (unlikely(vif->disabled))
+		if (unlikely(vif->disabled)) {
 			xenvif_carrier_off(vif);
-		else if (unlikely(test_and_clear_bit(VIF_STATUS_RX_PURGE_EVENT,
-						     &vif->status)))
-			xenvif_rx_purge_event(vif);
-
-		if (kthread_should_stop())
-			break;
+			xenvif_rx_queue_purge(vif);
+			continue;
+		}
 
 		if (!skb_queue_empty(&vif->rx_queue))
 			xenvif_rx_action(vif);
 
+		/* If the guest hasn't provided any Rx slots for a
+		 * while it's probably not responsive, drop the
+		 * carrier so packets are dropped earlier.
+		 */
+		if (xenvif_rx_queue_stalled(vif)) {
+			if (net_ratelimit())
+				netdev_info(vif->dev, "Guest Rx stalled");
+			netif_carrier_off(vif->dev);
+		} else if (xenvif_rx_queue_ready(vif)) {
+			if (net_ratelimit())
+				netdev_info(vif->dev, "Guest Rx ready");
+			netif_carrier_on(vif->dev);
+		}
+
+		/* Queued packets may have foreign pages from other
+		 * domains.  These cannot be queued indefinitely as
+		 * this would starve guests of grant refs and transmit
+		 * slots.
+		 */
+		xenvif_rx_queue_drop_expired(vif);
+
+		xenvif_rx_queue_maybe_wake(vif);
+
 		cond_resched();
 	}
 
 	/* Bin any remaining skbs */
-	while ((skb = skb_dequeue(&vif->rx_queue)) != NULL)
-		dev_kfree_skb(skb);
+	xenvif_rx_queue_purge(vif);
 
 	return 0;
 }
@@ -2051,6 +2144,7 @@ static int __init netback_init(void)
 		goto failed_init;
 
 	rx_drain_timeout_jiffies = msecs_to_jiffies(rx_drain_timeout_msecs);
+	rx_stall_timeout_jiffies = msecs_to_jiffies(rx_stall_timeout_msecs);
 
 #ifdef CONFIG_DEBUG_FS
 	xen_netback_dbg_root = debugfs_create_dir("xen-netback", NULL);
diff --git a/drivers/net/xen-netback/xenbus.c b/drivers/net/xen-netback/xenbus.c
index 375c2b9..5606096 100644
--- a/drivers/net/xen-netback/xenbus.c
+++ b/drivers/net/xen-netback/xenbus.c
@@ -110,6 +110,11 @@ static int xenvif_read_io_ring(struct seq_file *m, void *v)
 		   vif->credit_timeout.expires,
 		   jiffies);
 
+	seq_printf(m, "\nRx internal queue: len %u max %u pkts %u %s\n",
+		   vif->rx_queue_len, vif->rx_queue_max,
+		   skb_queue_len(&vif->rx_queue),
+		   netif_queue_stopped(vif->dev) ? "stopped" : "running");
+
 	return 0;
 }
 
