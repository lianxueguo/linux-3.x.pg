diff -aur XenGT-Server-kernel-drop2/drivers/gpu/drm/i915/vgt/aperture_gm.c XenGT-Server-kernel/drivers/gpu/drm/i915/vgt/aperture_gm.c
--- XenGT-Server-kernel-drop2/drivers/gpu/drm/i915/vgt/aperture_gm.c	2015-07-03 09:21:34.956175059 +0100
+++ XenGT-Server-kernel/drivers/gpu/drm/i915/vgt/aperture_gm.c	2015-07-03 09:24:39.169896438 +0100
@@ -208,6 +208,7 @@
 	unsigned long aperture_search_start = 0;
 	unsigned long visable_gm_start, hidden_gm_start = guard;
 	unsigned long fence_base;
+	int i=0;
 
 	ASSERT(vgt->aperture_base == 0); /* not allocated yet*/
 	ASSERT(vp.aperture_sz > 0 && vp.aperture_sz <= vp.gm_sz);
@@ -242,6 +243,13 @@
 	if (vp.gm_sz > vp.aperture_sz)
 		bitmap_set(gm_bitmap, hidden_gm_start, vp.gm_sz - vp.aperture_sz);
 	bitmap_set(fence_bitmap, fence_base, vp.fence_sz);
+
+	for (i = vgt->fence_base; i < vgt->fence_base + vgt->fence_sz; i++){
+		VGT_MMIO_WRITE_BYTES(pdev,
+			_REG_FENCE_0_LOW + 8 * i,
+			0, 8);
+	}
+
 	return 0;
 }
 
@@ -253,6 +261,7 @@
 	unsigned long visable_gm_start =
 		aperture_2_gm(vgt->pdev, vgt->aperture_base)/SIZE_1MB;
 	unsigned long hidden_gm_start = vgt->hidden_gm_offset/SIZE_1MB;
+	int i=0;
 
 	ASSERT(vgt->aperture_sz > 0 && vgt->aperture_sz <= vgt->gm_sz);
 
@@ -262,6 +271,12 @@
 		bitmap_clear(gm_bitmap, hidden_gm_start,
 			(vgt->gm_sz - vgt->aperture_sz)/SIZE_1MB);
 	bitmap_clear(fence_bitmap, vgt->fence_base,  vgt->fence_sz);
+
+	for (i = vgt->fence_base; i < vgt->fence_base + vgt->fence_sz; i++){
+		VGT_MMIO_WRITE_BYTES(pdev,
+			_REG_FENCE_0_LOW + 8 * i,
+			0, 8);
+	}
 }
 
 int alloc_vm_rsvd_aperture(struct vgt_device *vgt)
diff -aur XenGT-Server-kernel-drop2/drivers/gpu/drm/i915/vgt/cmd_parser.c XenGT-Server-kernel/drivers/gpu/drm/i915/vgt/cmd_parser.c
--- XenGT-Server-kernel-drop2/drivers/gpu/drm/i915/vgt/cmd_parser.c	2015-07-03 09:21:34.967175220 +0100
+++ XenGT-Server-kernel/drivers/gpu/drm/i915/vgt/cmd_parser.c	2015-07-03 09:24:39.184896652 +0100
@@ -165,14 +165,14 @@
 	}
 
 	entry = &list->handler[next];
+	/* two pages mapping are always valid */
+	memcpy(&entry->exec_state, s, sizeof(struct parser_exec_state));
 	/*
 	 * Do not use ip buf in post handle entry,
 	 * as ip buf has been freed at that time.
 	 * Switch back to guest memory write/read method
 	 */
 	entry->exec_state.ip_buf = entry->exec_state.ip_buf_va = NULL;
-	/* two pages mapping are always valid */
-	memcpy(&entry->exec_state, s, sizeof(struct parser_exec_state));
 	entry->handler = handler;
 	entry->request_id = s->request_id;
 
@@ -567,9 +567,26 @@
 	if (s->ip_va == NULL) {
 		vgt_err(" ip_va(NULL)\n");
 	} else {
+		int cnt = 0;
 		vgt_err("  ip_va=%p: %08x %08x %08x %08x \n",
 				s->ip_va, cmd_val(s, 0), cmd_val(s, 1), cmd_val(s, 2), cmd_val(s, 3));
+
 		vgt_print_opcode(cmd_val(s, 0), s->ring_id);
+
+		/* print the whole page to trace */
+		trace_printk("ERROR ip_va=%p: %08x %08x %08x %08x \n",
+				s->ip_va, cmd_val(s, 0), cmd_val(s, 1), cmd_val(s, 2), cmd_val(s, 3));
+
+		s->ip_va = (uint32_t*)((((u64)s->ip_va) >> 12) << 12);
+		while(cnt < 1024) {
+		trace_printk("DUMP ip_va=%p: %08x %08x %08x %08x %08x %08x %08x %08x \n",
+				s->ip_va, cmd_val(s, 0), cmd_val(s, 1), cmd_val(s, 2), cmd_val(s, 3),
+				          cmd_val(s, 4), cmd_val(s, 5), cmd_val(s, 6), cmd_val(s, 7));
+
+			s->ip_va+=8;
+			cnt+=8;
+		}
+
 	}
 }
 #define RING_BUF_WRAP(s, ip_gma)	(((s)->buf_type == RING_BUFFER_INSTRUCTION) && \
@@ -1276,7 +1293,7 @@
 static unsigned long get_gma_bb_from_cmd(struct parser_exec_state *s, int index)
 {
 	unsigned long addr;
-	int32_t gma_high, gma_low;
+	unsigned long gma_high, gma_low;
 	int gmadr_bytes = s->vgt->pdev->device_info.gmadr_bytes_in_cmd;
 
 	ASSERT(gmadr_bytes == 4 || gmadr_bytes == 8);
diff -aur XenGT-Server-kernel-drop2/drivers/gpu/drm/i915/vgt/execlists.c XenGT-Server-kernel/drivers/gpu/drm/i915/vgt/execlists.c
--- XenGT-Server-kernel-drop2/drivers/gpu/drm/i915/vgt/execlists.c	2015-07-03 09:21:35.000175706 +0100
+++ XenGT-Server-kernel/drivers/gpu/drm/i915/vgt/execlists.c	2015-07-03 09:24:39.184896652 +0100
@@ -1019,35 +1019,39 @@
 
 	status.execlist_write_pointer = (el_index == 0 ? 1 : 0);
 
-	/* TODO
-	 * 1, Check whether we should set below two states. According to the observation
-	 * from dom0, when there is ELSP write, both active bit and valid bit will be
-	 * set.
-	 * 2, Consider the emulation of preemption and lite restore.
-	 * It is designed to be in context switch by adding corresponding status entries
-	 * into status buffer.
-	 */
-	if (el_index == 0) {
-		status.execlist_0_active = 1;
-		status.execlist_0_valid = 1;
-	} else {
-		status.execlist_1_active = 1;
-		status.execlist_1_valid = 1;
-	}
+	if (status.execlist_0_valid == 0 && status.execlist_1_valid == 0) {
 
-	/* TODO emulate the status. Need double confirm
-	 *
-	 * Here non-render owner will not receive context switch interrupt
-	 * until it becomes a render owner. Meanwhile, the status register
-	 * is emulated to reflect the port submission operation.
-	 * It is noticed that the initial value of "current_execlist_pointer"
-	 * and "execlist_write_pointer" does not equal although the EXECLISTS
-	 * are all empty. It is then not appropriate to emulate "execlist_queue_full"
-	 * with the two bit value. Instead, the "execlist_queue_full" will be
-	 * set if valid bits of both "EXECLIST 0" and "EXECLIST 1" are set.
-	 * This needs the double confirm.
-	 */
-	if (status.execlist_0_valid && status.execlist_1_valid) {
+		status.udw = ctx0->guest_context.context_id;
+
+		/* TODO
+		 * 1, Check whether we should set below two states. According to the observation
+		 * from dom0, when there is ELSP write, both active bit and valid bit will be
+		 * set.
+		 * 2, Consider the emulation of preemption and lite restore.
+		 * It is designed to be in context switch by adding corresponding status entries
+		 * into status buffer.
+		 */
+		if (el_index == 0) {
+			status.execlist_0_active = 1;
+			status.execlist_0_valid = 1;
+			status.execlist_1_active = 0;
+			status.execlist_1_valid = 0;
+		} else {
+			status.execlist_0_active = 0;
+			status.execlist_0_valid = 0;
+			status.execlist_1_active = 1;
+			status.execlist_1_valid = 1;
+		}
+		/*update cur pointer to next */
+		status.current_execlist_pointer = el_index;
+	}
+	else {
+		/* TODO emulate the status. Need double confirm
+		 *
+		 * Here non-render owner will still receive context switch interrupt
+		 * injected because of HW GPU status change. Meanwhile, the status register
+		 * is emulated to reflect the port submission operation.
+		 */
 		status.execlist_queue_full = 1;
 		vgt_dbg(VGT_DBG_EXECLIST,"VM-%d: ring(%d) EXECLISTS becomes "
 			"full due to workload submission!\n",
@@ -1057,6 +1061,8 @@
 
 	__vreg(vgt, status_reg) = status.ldw;
 	__vreg(vgt, status_reg + 4) = status.udw;
+
+	return;
 }
 
 struct execlist_context * execlist_context_find(struct vgt_device *vgt,
@@ -1342,6 +1348,7 @@
 
 	ASSERT(ctx0 && ctx1);
 
+	ppgtt_check_partial_access(vgt);
 	ppgtt_sync_oos_pages(vgt);
 
 	vgt_dbg(VGT_DBG_EXECLIST, "EXECLIST is submitted into hardware! "
@@ -1410,12 +1417,26 @@
 	vgt->rb[ring_id].has_ppgtt_mode_enabled = 1;
 	vgt->rb[ring_id].has_ppgtt_base_set = 1;
 	vgt->rb[ring_id].request_id = el_ctx->request_id;
-	vgt->rb[ring_id].last_scan_head = el_ctx->last_scan_head;
-	if (!IS_PREEMPTION_RESUBMISSION(vring->head, vring->tail, el_ctx->last_scan_head)) {
+
+#if 0
+	/* keep this trace for debug purpose */
+	trace_printk("VRING: HEAD %04x TAIL %04x START %08x last_scan %08x PREEMPTION %d DPY %d\n",
+		vring->head, vring->tail, vring->start, el_ctx->last_scan_head,
+		IS_PREEMPTION_RESUBMISSION(vring->head, vring->tail,
+		el_ctx->last_scan_head), current_foreground_vm(vgt->pdev) == vgt);
+#endif
+	if (el_ctx->last_guest_head == vring->head) {
+		/* For lite-restore case from Guest, Headers are fixed,
+		 HW only resample tail */
 		vgt->rb[ring_id].last_scan_head = el_ctx->last_scan_head;
-		vgt_scan_vring(vgt, ring_id);
+	}
+	else {
+		vgt->rb[ring_id].last_scan_head = vring->head;
+		el_ctx->last_guest_head = vring->head;
 	}
 
+	vgt_scan_vring(vgt, ring_id);
+
 	/* the function is used to update ring/buffer only. No real submission inside */
 	vgt_submit_commands(vgt, ring_id);
 
@@ -1448,6 +1469,9 @@
 	struct execlist_status_format el_status;
 	uint32_t ctx_ptr_reg;
 	struct ctx_st_ptr_format ctx_st_ptr;
+	struct context_status_format ctx_status;
+	uint32_t ctx_status_reg = el_ring_mmio(ring_id, _EL_OFFSET_STATUS_BUF);
+	unsigned long last_csb_reg_offset;
 
 	el_ring_base = vgt_ring_id_to_EL_base(ring_id);
 	el_status_reg = el_ring_base + _EL_OFFSET_STATUS;
@@ -1460,13 +1484,18 @@
 	ctx_ptr_reg = el_ring_mmio(ring_id, _EL_OFFSET_STATUS_PTR);
 	ctx_st_ptr.dw = VGT_MMIO_READ(pdev, ctx_ptr_reg);
 
-	if (ctx_st_ptr.status_buf_write_ptr == DEFAULT_INV_SR_PTR
-			|| ctx_st_ptr.status_buf_read_ptr == DEFAULT_INV_SR_PTR)
+	if (ctx_st_ptr.status_buf_write_ptr == DEFAULT_INV_SR_PTR)
 		return true;
 
 	if (ctx_st_ptr.status_buf_read_ptr != ctx_st_ptr.status_buf_write_ptr)
 		return false;
 
+	last_csb_reg_offset = ctx_status_reg + ctx_st_ptr.status_buf_write_ptr * 8;
+	READ_STATUS_MMIO(pdev, last_csb_reg_offset, ctx_status);
+
+	if (!ctx_status.active_to_idle)
+		return false;
+
 	return true;
 }
 
@@ -1571,7 +1600,6 @@
 	ctx_descs[1] = (struct ctx_desc_format *)&elsp_store->element[0];
 
 	elsp_store->count = 0;
-	vgt_enable_ring(vgt, ring_id);
 
 	if (hvm_render_owner) {
 		uint32_t elsp_reg;
@@ -1726,3 +1754,30 @@
 	vgt_destroy_execlist_context(vgt, el_ctx);
 	return rc;
 }
+
+void vgt_reset_execlist(struct vgt_device *vgt, unsigned long ring_bitmap)
+{
+	vgt_state_ring_t *rb;
+	int bit, i;
+
+	for_each_set_bit(bit, &ring_bitmap, sizeof(ring_bitmap)) {
+		if (bit >= vgt->pdev->max_engines)
+			break;
+
+		rb = &vgt->rb[bit];
+
+		memset(&rb->vring, 0, sizeof(vgt_ringbuffer_t));
+		memset(&rb->sring, 0, sizeof(vgt_ringbuffer_t));
+
+		vgt_disable_ring(vgt, bit);
+
+		memset(&rb->elsp_store, 0, sizeof(rb->elsp_store));
+
+		rb->el_slots_head = rb->el_slots_tail = 0;
+		for (i = 0; i < EL_QUEUE_SLOT_NUM; ++ i)
+			memset(&rb->execlist_slots[i], 0,
+					sizeof(struct vgt_exec_list));
+
+		rb->csb_write_ptr = DEFAULT_INV_SR_PTR;
+	}
+}
diff -aur XenGT-Server-kernel-drop2/drivers/gpu/drm/i915/vgt/gtt.c XenGT-Server-kernel/drivers/gpu/drm/i915/vgt/gtt.c
--- XenGT-Server-kernel-drop2/drivers/gpu/drm/i915/vgt/gtt.c	2015-07-03 09:21:35.046176392 +0100
+++ XenGT-Server-kernel/drivers/gpu/drm/i915/vgt/gtt.c	2015-07-03 09:24:39.185896668 +0100
@@ -531,6 +531,9 @@
 
 	if (guest_page->writeprotection)
 		hypervisor_unset_wp_pages(vgt, guest_page);
+
+	if (guest_page == vgt->gtt.last_partial_ppgtt_access_gpt)
+		vgt->gtt.last_partial_ppgtt_access_index = -1;
 }
 
 guest_page_t *vgt_find_guest_page(struct vgt_device *vgt, unsigned long gfn)
@@ -1140,6 +1143,28 @@
 		&& gpt->write_cnt >= 2;
 }
 
+bool ppgtt_check_partial_access(struct vgt_device *vgt)
+{
+	struct vgt_vgtt_info *gtt = &vgt->gtt;
+
+	if (gtt->last_partial_ppgtt_access_index == -1)
+		return true;
+
+	if (!gtt->warn_partial_ppgtt_access_once) {
+		vgt_warn("Incomplete PPGTT page table access sequence.\n");
+		gtt->warn_partial_ppgtt_access_once = true;
+	}
+
+	if (!ppgtt_handle_guest_write_page_table(
+			gtt->last_partial_ppgtt_access_gpt,
+			&gtt->last_partial_ppgtt_access_entry,
+			gtt->last_partial_ppgtt_access_index))
+		return false;
+
+	gtt->last_partial_ppgtt_access_index = -1;
+	return true;
+}
+
 static bool ppgtt_handle_guest_write_page_table_bytes(void *gp,
 		uint64_t pa, void *p_data, int bytes)
 {
@@ -1148,6 +1173,7 @@
 	struct vgt_device *vgt = spt->vgt;
 	struct vgt_gtt_pte_ops *ops = vgt->pdev->gtt.pte_ops;
 	struct vgt_device_info *info = &vgt->pdev->device_info;
+	struct vgt_vgtt_info *gtt = &vgt->gtt;
 	gtt_entry_t we, se;
 	unsigned long index;
 
@@ -1163,6 +1189,8 @@
 		trace_gpt_change(vgt->vm_id, "partial access - LOW",
 				NULL, we.type, *(u32 *)(p_data), index);
 
+		ppgtt_check_partial_access(vgt);
+
 		ppgtt_set_guest_entry(spt, &we, index);
 		ppgtt_get_shadow_entry(spt, &se, index);
 
@@ -1175,8 +1203,14 @@
 
 		se.val64 = 0;
 		ppgtt_set_shadow_entry(spt, &se, index);
+
+		gtt->last_partial_ppgtt_access_index = index;
+		gtt->last_partial_ppgtt_access_gpt = gpt;
+		gtt->last_partial_ppgtt_access_entry = we;
+
 		return true;
-	}
+	} else
+		gtt->last_partial_ppgtt_access_index = -1;
 
 	if (hi)
 		trace_gpt_change(vgt->vm_id, "partial access - HIGH",
@@ -1919,6 +1953,8 @@
 	INIT_LIST_HEAD(&gtt->mm_list_head);
 	INIT_LIST_HEAD(&gtt->oos_page_list_head);
 
+	gtt->last_partial_ppgtt_access_index = -1;
+
 	if (!vgt_expand_shadow_page_mempool(vgt->pdev)) {
 		vgt_err("fail to expand the shadow page mempool.");
 		return false;
diff -aur XenGT-Server-kernel-drop2/drivers/gpu/drm/i915/vgt/handlers.c XenGT-Server-kernel/drivers/gpu/drm/i915/vgt/handlers.c
--- XenGT-Server-kernel-drop2/drivers/gpu/drm/i915/vgt/handlers.c	2015-07-03 09:21:35.068176712 +0100
+++ XenGT-Server-kernel/drivers/gpu/drm/i915/vgt/handlers.c	2015-07-03 09:24:39.185896668 +0100
@@ -685,6 +685,9 @@
 		vgt->rb[ring_id].has_execlist_enabled = ring_execlist;
 		vgt_info("EXECLIST %s on ring %d.\n",
 			(ring_execlist ? "enabling" : "disabling"), ring_id);
+
+		if (ring_execlist)
+			vgt_enable_ring(vgt, ring_id);
 	}
 
 	ring_ppgtt_mode(vgt, ring_id, off, mode);
@@ -3545,7 +3548,7 @@
 {0xb118, 4, F_RDR, 0, D_BDW, NULL, NULL},
 {0xb100, 4, F_RDR, 0, D_BDW, NULL, NULL},
 {0xb10c, 4, F_RDR, 0, D_BDW, NULL, NULL},
-{0xb110, 4, F_RDR, 0, D_BDW, NULL, NULL},
+{0xb110, 4, F_PT, 0, D_BDW, NULL, NULL},
 
 /* NON-PRIV */
 {0x24d0, 4, F_RDR, 0, D_BDW, NULL, NULL},
diff -aur XenGT-Server-kernel-drop2/drivers/gpu/drm/i915/vgt/instance.c XenGT-Server-kernel/drivers/gpu/drm/i915/vgt/instance.c
--- XenGT-Server-kernel-drop2/drivers/gpu/drm/i915/vgt/instance.c	2015-07-03 09:21:35.031176163 +0100
+++ XenGT-Server-kernel/drivers/gpu/drm/i915/vgt/instance.c	2015-07-03 09:24:39.186896683 +0100
@@ -314,8 +314,7 @@
 	*ptr_vgt = vgt;
 
 	/* initialize context scheduler infor */
-	if (event_based_qos)
-		vgt_init_sched_info(vgt);
+	vgt_init_sched_info(vgt);
 
 	if (shadow_tail_based_qos)
 		vgt_init_rb_tailq(vgt);
@@ -451,7 +450,6 @@
 
 void vgt_reset_ppgtt(struct vgt_device *vgt, unsigned long ring_bitmap)
 {
-	struct vgt_mm *mm;
 	int bit;
 
 	if (!vgt->pdev->enable_ppgtt || !vgt->gtt.active_ppgtt_mm_bitmap)
@@ -467,8 +465,6 @@
 		if (!test_bit(bit, &vgt->gtt.active_ppgtt_mm_bitmap))
 			continue;
 
-		mm = vgt->rb[bit].active_ppgtt_mm;
-
 		vgt_info("VM %d: Reset ring %d PPGTT state.\n", vgt->vm_id, bit);
 
 		vgt->rb[bit].has_ppgtt_mode_enabled = 0;
@@ -476,9 +472,6 @@
 		vgt->rb[bit].ppgtt_page_table_level = 0;
 		vgt->rb[bit].ppgtt_root_pointer_type = GTT_TYPE_INVALID;
 
-		vgt_destroy_mm(mm);
-
-		vgt->rb[bit].active_ppgtt_mm = NULL;
 		clear_bit(bit, &vgt->gtt.active_ppgtt_mm_bitmap);
 	}
 
@@ -491,7 +484,6 @@
 	int bit;
 
 	for_each_set_bit(bit, &ring_bitmap, sizeof(ring_bitmap)) {
-		int i;
 		if (bit >= vgt->pdev->max_engines)
 			break;
 
@@ -503,14 +495,8 @@
 		rb->uhptr = 0;
 		rb->request_id = rb->uhptr_id = 0;
 
-		rb->el_slots_head = rb->el_slots_tail = 0;
-		for (i = 0; i < EL_QUEUE_SLOT_NUM; ++ i)
-			memset(&rb->execlist_slots[i], 0,
-				sizeof(struct vgt_exec_list));
-
 		memset(&rb->vring, 0, sizeof(vgt_ringbuffer_t));
 		memset(&rb->sring, 0, sizeof(vgt_ringbuffer_t));
-		rb->csb_write_ptr = DEFAULT_INV_SR_PTR;
 
 		vgt_disable_ring(vgt, bit);
 
@@ -533,7 +519,10 @@
 {
 	ASSERT(spin_is_locked(&vgt->pdev->lock));
 
-	vgt_reset_ringbuffer(vgt, ring_bitmap);
+	if (!vgt->pdev->enable_execlist)
+		vgt_reset_ringbuffer(vgt, ring_bitmap);
+	else
+		vgt_reset_execlist(vgt, ring_bitmap);
 
 	vgt_reset_ppgtt(vgt, ring_bitmap);
 
diff -aur XenGT-Server-kernel-drop2/drivers/gpu/drm/i915/vgt/render.c XenGT-Server-kernel/drivers/gpu/drm/i915/vgt/render.c
--- XenGT-Server-kernel-drop2/drivers/gpu/drm/i915/vgt/render.c	2015-07-03 09:21:35.000175707 +0100
+++ XenGT-Server-kernel/drivers/gpu/drm/i915/vgt/render.c	2015-07-03 09:24:39.186896683 +0100
@@ -1683,7 +1683,6 @@
 
 static struct reg_mask_t gen8_rcs_reset_mmio[] = {
 	{0x2098, 0},
-	{0x229c, 1},
 	{0x20c0, 1},
 
 	{0x24d0, 0},
@@ -1705,8 +1704,6 @@
 
 	{0x7010, 1},
 
-	{0xb110, 0},
-
 	{0x83a4, 1},
 };
 
diff -aur XenGT-Server-kernel-drop2/drivers/gpu/drm/i915/vgt/sched.c XenGT-Server-kernel/drivers/gpu/drm/i915/vgt/sched.c
--- XenGT-Server-kernel-drop2/drivers/gpu/drm/i915/vgt/sched.c	2015-07-03 09:21:35.045176372 +0100
+++ XenGT-Server-kernel/drivers/gpu/drm/i915/vgt/sched.c	2015-07-03 09:24:39.186896683 +0100
@@ -37,11 +37,10 @@
 	struct vgt_hrtimer *hrtimer = &vgt_hrtimer;
 	hrtimer_init(&hrtimer->timer, CLOCK_MONOTONIC, HRTIMER_MODE_ABS);
 	hrtimer->timer.function = function;
-	hrtimer->period = period;
 	vgt_hrtimer_pdev = pdev;
 
 	hrtimer_start(&hrtimer->timer,
-			ktime_add_ns(ktime_get(), hrtimer->period),
+			ktime_add_ns(ktime_get(), period),
 			HRTIMER_MODE_ABS);
 }
 
@@ -114,7 +113,12 @@
 	if (vgt_nr_in_runq(pdev) > 1) {
 		vgt_raise_request(pdev, VGT_REQUEST_SCHED);
 	}
-	hrtimer_add_expires_ns(&hrtimer->timer, hrtimer->period);
+	/* we are safe to schedule next timeout with current vgt value
+	 * (before ctx switch). If ctx switch successfully, we will cancel
+	 * this timer and start new one with next vgt's tbs_period.
+	 */
+	hrtimer_add_expires_ns(&hrtimer->timer,
+		ctx_tbs_period(current_render_owner(pdev)));
 	return HRTIMER_RESTART;
 }
 
@@ -456,6 +460,7 @@
 			struct vgt_hrtimer, timer);
 	struct pgt_device *pdev = vgt_hrtimer_pdev;
 	int cpu;
+	u64 hrtimer_period = VGT_TAILQ_RB_POLLING_PERIOD;
 
 	ASSERT(pdev);
 
@@ -490,11 +495,11 @@
 	 */
 	if (vgt_removal_req == true) {
 		vgt_removal_req = false;
-		hrtimer->period = (VGT_TAILQ_RB_POLLING_PERIOD << 3);
+		hrtimer_period = (VGT_TAILQ_RB_POLLING_PERIOD << 3);
 	} else
-		hrtimer->period = VGT_TAILQ_RB_POLLING_PERIOD;
+		hrtimer_period = VGT_TAILQ_RB_POLLING_PERIOD;
 
-	hrtimer_add_expires_ns(&hrtimer->timer, hrtimer->period);
+	hrtimer_add_expires_ns(&hrtimer->timer, hrtimer_period);
 
 	return HRTIMER_RESTART;
 }
@@ -522,9 +527,10 @@
 		timer_based_qos = false;
 
 	if (timer_based_qos) {
+		ASSERT(current_render_owner(pdev));
 		vgt_hrtimer_init(pdev,
 				vgt_tbs_timer_fn,
-				VGT_TBS_DEFAULT_PERIOD);
+				ctx_tbs_period(current_render_owner(pdev)));
 	}
 }
 
@@ -777,7 +783,7 @@
 {
 	if (timer_based_qos)
 		hrtimer_start(&vgt_hrtimer.timer,
-			ktime_add_ns(ktime_get(), vgt_hrtimer.period),
+			ktime_add_ns(ktime_get(), ctx_tbs_period(vgt)),
 			HRTIMER_MODE_ABS);
 
 	/* setup countdown for next vgt context */
@@ -872,6 +878,6 @@
 {
 	vgt->force_removal = 1;
 	vgt->pdev->next_sched_vgt = vgt_dom0;
-	vgt_raise_request(vgt->pdev, VGT_REQUEST_SCHED);
+	vgt_raise_request(vgt->pdev, VGT_REQUEST_CTX_SWITCH);
 	wmb();
 }
diff -aur XenGT-Server-kernel-drop2/drivers/gpu/drm/i915/vgt/vgt.c XenGT-Server-kernel/drivers/gpu/drm/i915/vgt/vgt.c
--- XenGT-Server-kernel-drop2/drivers/gpu/drm/i915/vgt/vgt.c	2015-07-03 09:21:35.044176352 +0100
+++ XenGT-Server-kernel/drivers/gpu/drm/i915/vgt/vgt.c	2015-07-03 09:24:39.186896683 +0100
@@ -77,6 +77,10 @@
 module_param_named(event_based_qos, event_based_qos, bool, 0600);
 MODULE_PARM_DESC(event_based_qos, "Use event based QoS scheduler (default: false)");
 
+int tbs_period_ms = -1;
+module_param_named(tbs_period_ms, tbs_period_ms, int, 0600);
+MODULE_PARM_DESC(event_based_qos, "Set the time based QoS scheduler timer in unit of ms (default: BDW 1ms, HSW 15ms)");
+
 bool shadow_tail_based_qos = false;
 module_param_named(shadow_tail_based_qos, shadow_tail_based_qos, bool, 0600);
 MODULE_PARM_DESC(shadow_tail_based_qos, "Use Shadow tail based QoS scheduler (default: false)");
@@ -1022,28 +1026,32 @@
 		ASSERT(0);
 	}
 
-	vgt_info("GPU ring status:\n");
-
-	for (i = 0; i < pdev->max_engines; i++) {
-		head = VGT_READ_HEAD(pdev, i);
-		tail = VGT_READ_TAIL(pdev, i);
-		start = VGT_READ_START(pdev, i);
-		ctl = VGT_READ_CTL(pdev, i);
+	if (IS_PREBDW(pdev)) {
+		vgt_info("GPU ring status:\n");
 
-		vgt_info("RING %d: H: %x T: %x S: %x C: %x.\n",
-				i, head, tail, start, ctl);
+		for (i = 0; i < pdev->max_engines; i++) {
+			head = VGT_READ_HEAD(pdev, i);
+			tail = VGT_READ_TAIL(pdev, i);
+			start = VGT_READ_START(pdev, i);
+			ctl = VGT_READ_CTL(pdev, i);
 
-		if (pdev->enable_execlist)
-			reset_el_structure(pdev, i);
-	}
+			vgt_info("RING %d: H: %x T: %x S: %x C: %x.\n",
+					i, head, tail, start, ctl);
+		}
 
-	ier = VGT_MMIO_READ(pdev, _REG_DEIER);
-	iir = VGT_MMIO_READ(pdev, _REG_DEIIR);
-	imr = VGT_MMIO_READ(pdev, _REG_DEIMR);
-	isr = VGT_MMIO_READ(pdev, _REG_DEISR);
+		ier = VGT_MMIO_READ(pdev, _REG_DEIER);
+		iir = VGT_MMIO_READ(pdev, _REG_DEIIR);
+		imr = VGT_MMIO_READ(pdev, _REG_DEIMR);
+		isr = VGT_MMIO_READ(pdev, _REG_DEISR);
 
-	vgt_info("DE: ier: %x iir: %x imr: %x isr: %x.\n",
-			ier, iir, imr, isr);
+		vgt_info("DE: ier: %x iir: %x imr: %x isr: %x.\n",
+				ier, iir, imr, isr);
+	} else {
+		for (i = 0; i < pdev->max_engines; i++) {
+			if (pdev->enable_execlist)
+				reset_el_structure(pdev, i);
+		}
+	}
 
 	vgt_info("Finish.\n");
 
@@ -1086,9 +1094,11 @@
 
 int vgt_reset_device(struct pgt_device *pdev)
 {
+	struct vgt_irq_host_state *hstate = pdev->irq_hstate;
 	struct vgt_device *vgt;
 	struct list_head *pos, *n;
-	unsigned long ier;
+	unsigned long ier_reg = IS_PREBDW(pdev) ? _REG_DEIER : _REG_MASTER_IRQ;
+	unsigned long ier_value;
 	unsigned long flags;
 	int i;
 
@@ -1137,8 +1147,7 @@
 
 	vgt_get_irq_lock(pdev, flags);
 
-	VGT_MMIO_WRITE(pdev, _REG_DEIER,
-			VGT_MMIO_READ(pdev, _REG_DEIER) & ~_REGBIT_MASTER_INTERRUPT);
+	hstate->ops->disable_irq(hstate);
 
 	vgt_put_irq_lock(pdev, flags);
 
@@ -1155,14 +1164,15 @@
 
 	reset_cached_interrupt_registers(pdev);
 
-	ier = vgt_recalculate_ier(pdev, _REG_DEIER);
-	VGT_MMIO_WRITE(pdev, _REG_DEIER, ier);
+	ier_value = vgt_recalculate_ier(pdev, ier_reg);
+	VGT_MMIO_WRITE(pdev, ier_reg, ier_value);
 
 	vgt_put_irq_lock(pdev, flags);
 
 	spin_unlock_irqrestore(&pdev->lock, flags);
 
-	vgt_info("Enable master interrupt, DEIER: %lx\n", ier);
+	vgt_info("Enable master interrupt, master ier register %lx value %lx\n",
+			ier_reg, ier_value);
 
 	return 0;
 }
diff -aur XenGT-Server-kernel-drop2/drivers/gpu/drm/i915/vgt/vgt.h XenGT-Server-kernel/drivers/gpu/drm/i915/vgt/vgt.h
--- XenGT-Server-kernel-drop2/drivers/gpu/drm/i915/vgt/vgt.h	2015-07-03 09:21:35.069176729 +0100
+++ XenGT-Server-kernel/drivers/gpu/drm/i915/vgt/vgt.h	2015-07-03 09:24:39.187896698 +0100
@@ -93,6 +93,8 @@
 extern int preallocated_oos_pages;
 extern bool spt_out_of_sync;
 extern bool cmd_parser_ip_buf;
+extern bool timer_based_qos;
+extern int tbs_period_ms;
 
 enum vgt_event_type {
 	// GT
@@ -607,6 +609,8 @@
 extern bool gen8_mm_alloc_page_table(struct vgt_mm *mm);
 extern void gen8_mm_free_page_table(struct vgt_mm *mm);
 
+struct guest_page;
+
 struct vgt_vgtt_info {
 	struct vgt_mm *ggtt_mm;
 	unsigned long active_ppgtt_mm_bitmap;
@@ -616,6 +620,10 @@
 	DECLARE_HASHTABLE(el_ctx_hash_table, VGT_HASH_BITS);
 	atomic_t n_write_protected_guest_page;
 	struct list_head oos_page_list_head;
+	int last_partial_ppgtt_access_index;
+	gtt_entry_t last_partial_ppgtt_access_entry;
+	struct guest_page *last_partial_ppgtt_access_gpt;
+	bool warn_partial_ppgtt_access_once;
 };
 
 extern bool vgt_init_vgtt(struct vgt_device *vgt);
@@ -632,6 +640,8 @@
 extern struct vgt_mm *gen8_find_ppgtt_mm(struct vgt_device *vgt,
                 int page_table_level, void *root_entry);
 
+extern bool ppgtt_check_partial_access(struct vgt_device *vgt);
+
 typedef bool guest_page_handler_t(void *gp, uint64_t pa, void *p_data, int bytes);
 
 struct oos_page;
@@ -696,6 +706,7 @@
 	 * data and store them into vgt->rb[ring_id] before a
 	 * context is submitted. We will have better handling later.
 	 */
+	vgt_reg_t last_guest_head;
 	vgt_reg_t last_scan_head;
 	uint64_t request_id;
 	//uint64_t cmd_nr;
@@ -816,13 +827,15 @@
 	int32_t weight;
 	int64_t time_slice;
 	/* more properties and policies should be added in*/
+	u64 tbs_period;  /* default: VGT_TBS_DEFAULT_PERIOD(1ms) */
 };
 
-#define VGT_TBS_DEFAULT_PERIOD (15 * 1000000) /* 15 ms */
+#define VGT_TBS_PERIOD_MAX 15
+#define VGT_TBS_PERIOD_MIN 1
+#define VGT_TBS_DEFAULT_PERIOD(x) ((x) * 1000000) /* 15 ms */
 
 struct vgt_hrtimer {
 	struct hrtimer timer;
-	u64 period;
 };
 
 #define VGT_TAILQ_RB_POLLING_PERIOD (2 * 1000000)
@@ -2150,6 +2163,7 @@
 #define ctx_remain_time(vgt) ((vgt)->sched_info.time_slice)
 #define ctx_actual_end_time(vgt) ((vgt)->sched_info.actual_end_time)
 #define ctx_rb_empty_delay(vgt) ((vgt)->sched_info.rb_empty_delay)
+#define ctx_tbs_period(vgt) ((vgt)->sched_info.tbs_period)
 
 #define vgt_get_cycles() ({		\
 	cycles_t __ret;				\
@@ -2204,11 +2218,35 @@
 
 static inline void vgt_init_sched_info(struct vgt_device *vgt)
 {
-	ctx_remain_time(vgt) = VGT_DEFAULT_TSLICE;
-	ctx_start_time(vgt) = 0;
-	ctx_end_time(vgt) = 0;
-	ctx_actual_end_time(vgt) = 0;
-	ctx_rb_empty_delay(vgt) = 0;
+	if (event_based_qos) {
+		ctx_remain_time(vgt) = VGT_DEFAULT_TSLICE;
+		ctx_start_time(vgt) = 0;
+		ctx_end_time(vgt) = 0;
+		ctx_actual_end_time(vgt) = 0;
+		ctx_rb_empty_delay(vgt) = 0;
+	}
+
+	if (timer_based_qos) {
+
+		if (tbs_period_ms == -1) {
+			tbs_period_ms = IS_BDW(vgt->pdev) ?
+				VGT_TBS_PERIOD_MIN : VGT_TBS_PERIOD_MAX;
+		}
+
+		if (tbs_period_ms > VGT_TBS_PERIOD_MAX
+			|| tbs_period_ms < VGT_TBS_PERIOD_MIN) {
+			vgt_err("Invalid tbs_period=%d parameters. "
+				"Best value between <%d..%d>\n",
+				VGT_TBS_PERIOD_MIN, VGT_TBS_PERIOD_MAX,
+				tbs_period_ms);
+			tbs_period_ms = IS_BDW(vgt->pdev) ?
+				VGT_TBS_PERIOD_MIN : VGT_TBS_PERIOD_MAX;
+		}
+
+		ctx_tbs_period(vgt) = VGT_TBS_DEFAULT_PERIOD(tbs_period_ms);
+		vgt_info("VM-%d setup timebased schedule period %d ms\n",
+			vgt->vm_id, tbs_period_ms);
+	}
 }
 
 /* main context scheduling process */
@@ -2674,6 +2712,7 @@
 
 void vgt_reset_virtual_states(struct vgt_device *vgt, unsigned long ring_bitmap);
 void vgt_reset_ppgtt(struct vgt_device *vgt, unsigned long ring_bitmap);
+void vgt_reset_execlist(struct vgt_device *vgt, unsigned long ring_bitmap);
 
 enum vgt_pipe get_edp_input(uint32_t wr_data);
 void vgt_forward_events(struct pgt_device *pdev);
diff -aur XenGT-Server-kernel-drop2/drivers/xen/xengt.c XenGT-Server-kernel/drivers/xen/xengt.c
--- XenGT-Server-kernel-drop2/drivers/xen/xengt.c	2015-07-03 09:21:38.510227551 +0100
+++ XenGT-Server-kernel/drivers/xen/xengt.c	2015-07-03 09:24:39.197896843 +0100
@@ -498,9 +498,9 @@
 	nr_high_4k_bkt = (info->vmem_sz >> PAGE_SHIFT);
 
 	info->vmem_vma_low_1mb =
-		kmalloc(sizeof(*info->vmem_vma) * nr_low_1mb_bkt, GFP_KERNEL);
+		vzalloc(sizeof(*info->vmem_vma) * nr_low_1mb_bkt);
 	info->vmem_vma =
-		kmalloc(sizeof(*info->vmem_vma) * nr_high_bkt, GFP_KERNEL);
+		vzalloc(sizeof(*info->vmem_vma) * nr_high_bkt);
 	info->vmem_vma_4k =
 		vzalloc(sizeof(*info->vmem_vma) * nr_high_4k_bkt);
 
@@ -566,8 +566,8 @@
 
 	return 0;
 err:
-	kfree(info->vmem_vma);
-	kfree(info->vmem_vma_low_1mb);
+	vfree(info->vmem_vma);
+	vfree(info->vmem_vma_low_1mb);
 	vfree(info->vmem_vma_4k);
 	info->vmem_vma = info->vmem_vma_low_1mb = info->vmem_vma_4k = NULL;
 	return -ENOMEM;
@@ -619,8 +619,8 @@
 			vgt->vm_id);
 	}
 
-	kfree(info->vmem_vma);
-	kfree(info->vmem_vma_low_1mb);
+	vfree(info->vmem_vma);
+	vfree(info->vmem_vma_low_1mb);
 	vfree(info->vmem_vma_4k);
 }
 
Only in XenGT-Server-kernel: .git
