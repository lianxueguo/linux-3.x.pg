diff --git a/arch/x86/include/asm/xen/hypercall.h b/arch/x86/include/asm/xen/hypercall.h
index 7e48263..a41b135 100644
--- a/arch/x86/include/asm/xen/hypercall.h
+++ b/arch/x86/include/asm/xen/hypercall.h
@@ -47,7 +47,7 @@
 #include <xen/interface/xen.h>
 #include <xen/interface/sched.h>
 #include <xen/interface/physdev.h>
-#include <xen/interface/iommu.h>
+#include <xen/interface/pv-iommu.h>
 #include <xen/interface/platform.h>
 #include <xen/interface/xen-mca.h>
 #include <xen/interface/domctl.h>
@@ -402,9 +402,9 @@ HYPERVISOR_grant_table_op(unsigned int cmd, void *uop, unsigned int count)
 }
 
 static inline int
-HYPERVISOR_iommu_op(unsigned int cmd, void *uop, unsigned int count)
+HYPERVISOR_iommu_op(void *uop, unsigned int count)
 {
-	return _hypercall3(int, iommu_op, cmd, uop, count);
+	return _hypercall2(int, iommu_op, uop, count);
 }
 
 static inline int
diff --git a/arch/x86/xen/pci-swiotlb-xen.c b/arch/x86/xen/pci-swiotlb-xen.c
index ef6283f..6eca385 100644
--- a/arch/x86/xen/pci-swiotlb-xen.c
+++ b/arch/x86/xen/pci-swiotlb-xen.c
@@ -30,7 +30,7 @@ bool pv_iommu_1_to_1_setup_complete;
 EXPORT_SYMBOL(pv_iommu_1_to_1_setup_complete);
 
 int xen_swiotlb __read_mostly;
-static struct iommu_map_op iommu_map_ops[IOMMU_BATCH_SIZE];
+static struct pv_iommu_op iommu_ops[IOMMU_BATCH_SIZE];
 static struct task_struct *xen_pv_iommu_setup_task;
 
 int xen_pv_iommu_map_sg_attrs(struct device *hwdev, struct scatterlist *sgl,
@@ -82,63 +82,52 @@ static struct dma_map_ops xen_pv_iommu_dma_ops = {
 	.sync_sg_for_device = swiotlb_sync_sg_for_device,
 };
 
-int xen_iommu_map_page(unsigned long pfn, unsigned long mfn)
+int xen_iommu_map_page(unsigned long bfn, unsigned long mfn)
 {
-	struct iommu_map_op iommu_op;
+	struct pv_iommu_op iommu_op;
 	int rc;
 
-	iommu_op.gmfn = pfn;
-	iommu_op.mfn = mfn;
-	iommu_op.flags = 3;
-	rc = HYPERVISOR_iommu_op(IOMMUOP_map_page, &iommu_op, 1);
+	iommu_op.u.map_page.bfn = bfn;
+	iommu_op.u.map_page.gfn = mfn;
+	iommu_op.flags = IOMMU_OP_readable | IOMMU_OP_writeable | IOMMU_MAP_OP_no_ref_cnt;
+	iommu_op.subop_id = IOMMUOP_map_page;
+	rc = HYPERVISOR_iommu_op(&iommu_op, 1);
 	if (rc < 0) {
 		printk("Failed to setup IOMMU mapping for gpfn 0x%lx, mfn 0x%lx, err %d\n",
-				pfn, mfn, rc);
+				bfn, mfn, rc);
 		return rc;
 	}
 	return iommu_op.status;
 }
 EXPORT_SYMBOL_GPL(xen_iommu_map_page);
 
-int xen_iommu_unmap_page(unsigned long pfn)
+int xen_iommu_unmap_page(unsigned long bfn)
 {
-	struct iommu_map_op iommu_op;
+	struct pv_iommu_op iommu_op;
 	int rc;
 
-	iommu_op.gmfn = pfn;
-	iommu_op.mfn = 0;
-	iommu_op.flags = 0;
-	rc = HYPERVISOR_iommu_op(IOMMUOP_unmap_page, &iommu_op, 1);
+	iommu_op.u.unmap_page.bfn = bfn;
+	iommu_op.flags = IOMMU_MAP_OP_no_ref_cnt;
+	iommu_op.subop_id = IOMMUOP_unmap_page;
+	rc = HYPERVISOR_iommu_op(&iommu_op, 1);
 	if (rc < 0) {
-		printk("Failed to remove IOMMU mapping for gpfn 0x%lx, err %d\n", pfn, rc);
+		printk("Failed to remove IOMMU mapping for gpfn 0x%lx, err %d\n", bfn, rc);
 		return rc;
 	}
 	return iommu_op.status;
 }
 
-int xen_iommu_batch_map(struct iommu_map_op *iommu_ops, int count)
+int xen_iommu_batch(struct pv_iommu_op *iommu_ops, int count)
 {
 	int rc;
 
-	rc = HYPERVISOR_iommu_op(IOMMUOP_map_page, iommu_ops, count);
+	rc = HYPERVISOR_iommu_op(iommu_ops, count);
 	if (rc < 0) {
 		printk("Failed to batch IOMMU map, err %d\n", rc);
 	}
 	return rc;
 }
-EXPORT_SYMBOL_GPL(xen_iommu_batch_map);
-
-int xen_iommu_batch_unmap(struct iommu_map_op *iommu_ops, int count)
-{
-	int rc;
-
-	rc = HYPERVISOR_iommu_op(IOMMUOP_unmap_page, iommu_ops, count);
-	if (rc < 0) {
-		printk("Failed to batch IOMMU unmap, err %d\n", rc);
-	}
-	return rc;
-}
-EXPORT_SYMBOL_GPL(xen_iommu_batch_unmap);
+EXPORT_SYMBOL_GPL(xen_iommu_batch);
 
 static int pv_iommu_setup(void *data)
 {
@@ -155,22 +144,22 @@ static int pv_iommu_setup(void *data)
 	{
 		if (get_phys_to_machine(i) == INVALID_P2M_ENTRY)
 		{
-			iommu_map_ops[count].gmfn = i;
-			iommu_map_ops[count].mfn = 0;
-			iommu_map_ops[count].flags = 0;
+			iommu_ops[count].u.unmap_page.bfn = i;
+			iommu_ops[count].flags = IOMMU_MAP_OP_no_ref_cnt;
+			iommu_ops[count].subop_id = IOMMUOP_unmap_page;
 			count++;
 		}
 		if (count == IOMMU_BATCH_SIZE)
 		{
 			count = 0;
-			if (xen_iommu_batch_unmap(iommu_map_ops,
+			if (xen_iommu_batch(iommu_ops,
 						IOMMU_BATCH_SIZE))
 				panic("Xen PV-IOMMU: failed to remove legacy"
 						" mappings\n");
 			cond_resched();
 		}
 	}
-	if (count && xen_iommu_batch_unmap(iommu_map_ops, count))
+	if (count && xen_iommu_batch(iommu_ops, count))
 		panic("Xen PV-IOMMU: failed to remove legacy mappings\n");
 
 	count = 0;
@@ -178,21 +167,24 @@ static int pv_iommu_setup(void *data)
 	 * top of host RAM */
 	for (i=0; i < max_host_mfn; i++)
 	{
-		iommu_map_ops[count].gmfn = max_host_mfn + i;
-		iommu_map_ops[count].mfn = i;
-		iommu_map_ops[count].flags = 3;
+		iommu_ops[count].u.map_page.bfn = max_host_mfn + i;
+		iommu_ops[count].u.map_page.gfn = i;
+		iommu_ops[count].flags = IOMMU_OP_readable | IOMMU_OP_writeable |
+					IOMMU_MAP_OP_no_ref_cnt |
+					IOMMU_MAP_OP_add_m2b;
+		iommu_ops[count].subop_id = IOMMUOP_map_page;
 		count++;
 		if (count == IOMMU_BATCH_SIZE)
 		{
 			count = 0;
-			if (xen_iommu_batch_map(iommu_map_ops,
+			if (xen_iommu_batch(iommu_ops,
 						IOMMU_BATCH_SIZE))
 				panic("Xen PV-IOMMU: failed to setup"
 					" 1-1 mapping\n");
 			cond_resched();
 		}
 	}
-	if (count && xen_iommu_batch_map(iommu_map_ops, count))
+	if (count && xen_iommu_batch(iommu_ops, count))
 		panic("Xen PV-IOMMU: failed to setup 1-1 mappings");
 
 	printk(KERN_INFO "XEN-PV-IOMMU - completed setting up 1-1 mapping\n");
@@ -208,13 +200,22 @@ static int pv_iommu_setup(void *data)
  */
 int __init pci_xen_swiotlb_detect(void)
 {
-
 	if (!xen_pv_domain())
 		return 0;
 
 	if (xen_initial_domain()){
-		int i, count = 0;
-		u64 max_mapped_gmfn, max_host_mfn = 0;
+		int i, j, count = 0;
+		u64 max_host_mfn = 0;
+		struct pv_iommu_op iommu_op;
+		int rc;
+
+		iommu_op.flags = 0;
+		iommu_op.status = 0;
+		iommu_op.subop_id = IOMMUOP_query_caps;
+		rc = HYPERVISOR_iommu_op(&iommu_op, 1);
+
+		if (rc || !(iommu_op.flags & IOMMU_QUERY_map_cap))
+			goto no_pv_iommu;
 
 		max_host_mfn = HYPERVISOR_memory_op(XENMEM_maximum_ram_page, NULL);
 		printk("Max host RAM MFN is 0x%llx\n",max_host_mfn);
@@ -226,23 +227,36 @@ int __init pci_xen_swiotlb_detect(void)
 			if ((get_phys_to_machine(i) != INVALID_P2M_ENTRY)
 					&& (i != pfn_to_mfn(i)))
 			{
-				iommu_map_ops[count].gmfn = i;
-				iommu_map_ops[count].mfn = pfn_to_mfn(i);
-				iommu_map_ops[count].flags = 3;
+				iommu_ops[count].u.unmap_page.bfn = i;
+				iommu_ops[count].flags = IOMMU_MAP_OP_no_ref_cnt;
+				iommu_ops[count].subop_id = IOMMUOP_unmap_page;
+				count++;
+				iommu_ops[count].u.map_page.bfn = i;
+				iommu_ops[count].u.map_page.gfn = pfn_to_mfn(i);
+				iommu_ops[count].flags = IOMMU_OP_readable |
+							IOMMU_OP_writeable |
+							IOMMU_MAP_OP_no_ref_cnt;
+				iommu_ops[count].subop_id = IOMMUOP_map_page;
 				count++;
-
-
 			}
 			if (count == IOMMU_BATCH_SIZE)
 			{
 				count = 0;
-				if (xen_iommu_batch_map(iommu_map_ops,
+				if (xen_iommu_batch(iommu_ops,
 							IOMMU_BATCH_SIZE))
 					goto remove_iommu_mappings;
+				for (j = 1; j < IOMMU_BATCH_SIZE; j +=2)
+				{
+					if ( iommu_ops[j].status )
+						printk("Iommu op %d went wrong,"
+								" subop id %d, bfn 0x%lx, gfn 0x%lx\n, err %d, flags 0x%x\n", j, iommu_ops[j].subop_id, 
+							       	iommu_ops[j].u.map_page.bfn, iommu_ops[j].u.map_page.gfn, iommu_ops[j].status, iommu_ops[j].flags );
+
+				}
 			}
 
 		}
-		if (count && xen_iommu_batch_map(iommu_map_ops, count))
+		if (count && xen_iommu_batch(iommu_ops, count))
 			goto remove_iommu_mappings;
 
 		/* Setup 1-1 host RAM offset location and hook the PV IOMMU DMA ops */
@@ -254,17 +268,10 @@ int __init pci_xen_swiotlb_detect(void)
 		return 1;
 
 remove_iommu_mappings:
-		if ( i != 0) {
-			printk("Failed to setup GPFN IOMMU mode\n");
-			max_mapped_gmfn = i;
-			for (i=0; i < max_mapped_gmfn; i++)
-				if (pfn_to_mfn(i) != INVALID_P2M_ENTRY)
-					if(xen_iommu_unmap_page(i))
-						printk("Failed to remove IOMMU"
-								" mapping\n");
-		}
+		BUG();
 	}
 
+no_pv_iommu:
 	/* If running as PV guest, either iommu=soft, or swiotlb=force will
 	 * activate this IOMMU. If running as PV privileged, activate it
 	 * irregardless.
